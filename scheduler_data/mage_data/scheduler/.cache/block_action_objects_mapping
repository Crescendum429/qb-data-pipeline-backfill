{"block_file": {"data_exporters/save_to_raw_invoices.py:data_exporter:python:save to raw invoices": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\nimport pandas as pd\nfrom datetime import datetime, timezone\n\n@data_exporter\ndef save_to_raw_invoices(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Data exporter para guardar INVOICES de QuickBooks en PostgreSQL raw\n    Implementa idempotencia, validaciones de calidad y m\u00e9tricas detalladas\n    \"\"\"\n    \n    print(\"=== QB INVOICES DATA EXPORT STARTED ===\")\n    \n    # Configuraci\u00f3n de base de datos - TABLA CORRECTA\n    schema_name = 'raw'\n    table_name = 'qb_invoices'  # CORRECTO: invoices, no customers\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    \n    print(f\"Target table: {schema_name}.{table_name}\")\n    print(f\"Input records: {len(df)}\")\n    \n    # Validaciones de calidad de datos\n    if len(df) == 0:\n        print(\"WARNING: No records to process - export aborted\")\n        return\n    \n    print(\"--- DATA QUALITY VALIDATIONS ---\")\n    \n    # 1. Verificar IDs no nulos\n    initial_count = len(df)\n    df = df[df['id'].notnull()]\n    null_ids_removed = initial_count - len(df)\n    if null_ids_removed > 0:\n        print(f\"VALIDATION: Removed {null_ids_removed} records with null IDs\")\n    \n    # 2. Verificar y remover duplicados\n    initial_count = len(df)\n    df = df.drop_duplicates(subset=['id'], keep='first')\n    duplicates_removed = initial_count - len(df)\n    if duplicates_removed > 0:\n        print(f\"VALIDATION: Removed {duplicates_removed} duplicate IDs\")\n    \n    # 3. Validar formato de IDs\n    invalid_ids = df[df['id'].astype(str).str.len() == 0]\n    if len(invalid_ids) > 0:\n        print(f\"VALIDATION: Found {len(invalid_ids)} records with empty IDs\")\n        df = df[df['id'].astype(str).str.len() > 0]\n    \n    print(f\"Records after validation: {len(df)}\")\n    \n    if len(df) == 0:\n        print(\"ERROR: No valid records remain after validation\")\n        return\n    \n    # Preparaci\u00f3n de datos para PostgreSQL\n    print(\"--- DATA PREPROCESSING ---\")\n    \n    # Obtener par\u00e1metros de runtime\n    fecha_inicio = kwargs.get('fecha_inicio', '2025-01-01')\n    fecha_fin = kwargs.get('fecha_fin', '2025-09-16')\n    \n    # Procesar columnas de timestamp\n    timestamp_columns = [\n        'ingested_at_utc', \n        'extract_window_start_utc', \n        'extract_window_end_utc'\n    ]\n    \n    current_utc = datetime.now(timezone.utc)\n    \n    for col in timestamp_columns:\n        if col in df.columns:\n            if col == 'ingested_at_utc':\n                df[col] = current_utc\n            elif col == 'extract_window_start_utc':\n                df[col] = pd.Timestamp(fecha_inicio, tz='UTC')\n            elif col == 'extract_window_end_utc':\n                df[col] = pd.Timestamp(fecha_fin, tz='UTC')\n            else:\n                df[col] = pd.to_datetime(df[col], utc=True)\n    \n    # Asegurar tipos de datos correctos\n    numeric_columns = ['page_number', 'page_size']\n    for col in numeric_columns:\n        if col in df.columns:\n            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n    \n    # Asegurar que JSON fields sean strings v\u00e1lidos\n    json_columns = ['payload', 'request_payload']\n    for col in json_columns:\n        if col in df.columns:\n            df[col] = df[col].astype(str)\n    \n    print(\"Data preprocessing completed\")\n    \n    # Implementar idempotencia con verificaci\u00f3n de registros existentes\n    print(\"--- IDEMPOTENCE CHECK ---\")\n    \n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        \n        try:\n            # Verificar registros existentes en la tabla CORRECTA\n            if len(df) > 0:\n                ids_list = [str(id_val) for id_val in df['id'].tolist()]\n                ids_str = \"','\".join(ids_list)\n                existing_query = f\"\"\"\n                    SELECT id FROM {schema_name}.{table_name} \n                    WHERE id IN ('{ids_str}')\n                \"\"\"\n                \n                existing_records = loader.load(existing_query)\n                existing_ids_set = set(existing_records['id'].tolist()) if len(existing_records) > 0 else set()\n                \n                # Filtrar solo registros nuevos\n                new_records_df = df[~df['id'].isin(existing_ids_set)]\n                existing_count = len(df) - len(new_records_df)\n                \n                print(f\"Existing records found: {existing_count}\")\n                print(f\"New records to insert: {len(new_records_df)}\")\n                \n                if len(new_records_df) > 0:\n                    print(\"--- DATABASE EXPORT ---\")\n                    \n                    # Insertar solo registros nuevos\n                    loader.export(\n                        new_records_df,\n                        schema_name,\n                        table_name,\n                        index=False,\n                        if_exists='append',\n                        allow_reserved_words=True\n                    )\n                    \n                    print(f\"Successfully inserted {len(new_records_df)} new invoice records\")\n                    \n                else:\n                    print(\"IDEMPOTENCE: All invoice records already exist - no inserts needed\")\n            \n        except Exception as e:\n            print(f\"ERROR during idempotence check: {e}\")\n            print(\"Falling back to standard append operation...\")\n            \n            # Fallback: insertar todo y dejar que la DB maneje duplicados\n            try:\n                loader.export(\n                    df,\n                    schema_name,\n                    table_name,\n                    index=False,\n                    if_exists='append',\n                    allow_reserved_words=True\n                )\n                print(f\"Fallback insert completed for {len(df)} invoice records\")\n            except Exception as fallback_error:\n                print(f\"CRITICAL ERROR: Fallback also failed: {fallback_error}\")\n                raise\n    \n    # Verificaci\u00f3n final y m\u00e9tricas\n    print(\"--- FINAL VERIFICATION AND METRICS ---\")\n    \n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        \n        # M\u00e9tricas generales de la tabla de invoices\n        metrics_query = f\"\"\"\n            SELECT \n                COUNT(*) as total_records,\n                COUNT(DISTINCT id) as unique_invoices,\n                MIN(ingested_at_utc) as first_ingested,\n                MAX(ingested_at_utc) as last_ingested,\n                COUNT(*) FILTER (WHERE DATE(ingested_at_utc) = CURRENT_DATE) as records_today\n            FROM {schema_name}.{table_name}\n        \"\"\"\n        \n        metrics_result = loader.load(metrics_query)\n        metrics = metrics_result.iloc[0]\n        \n        print(\"=== INVOICES EXPORT COMPLETION METRICS ===\")\n        print(f\"Total invoices in table: {metrics['total_records']}\")\n        print(f\"Unique invoice IDs: {metrics['unique_invoices']}\")\n        print(f\"Records processed this run: {len(df)}\")\n        print(f\"Records ingested today: {metrics['records_today']}\")\n        print(f\"First record ever: {metrics['first_ingested']}\")\n        print(f\"Last record: {metrics['last_ingested']}\")\n    \n    print(\"=== QB INVOICES DATA EXPORT COMPLETED ===\")\n    \n    return None", "file_path": "data_exporters/save_to_raw_invoices.py", "language": "python", "type": "data_exporter", "uuid": "save_to_raw_invoices"}, "data_exporters/save_to_raw_items_py.py:data_exporter:python:save to raw items py": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\nimport pandas as pd\n\n@data_exporter\ndef save_to_raw_items(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Data exporter para guardar items de QuickBooks en PostgreSQL raw\n    Implementa idempotencia y validaciones de calidad de datos\n    \"\"\"\n    \n    # Configuraci\u00f3n usando el perfil del io_config.yaml\n    schema_name = 'raw'\n    table_name = 'qb_items'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    \n    print(f'Saving {len(df)} records to {schema_name}.{table_name}...')\n    \n    # Validaciones de calidad de datos\n    if len(df) == 0:\n        print('Warning: No records to process')\n        return\n        \n    # Verificar que no hay IDs nulos\n    null_ids = df['id'].isnull().sum()\n    if null_ids > 0:\n        print(f'Warning: Found {null_ids} records with null IDs - removing them')\n        df = df[df['id'].notnull()]\n        \n    # Verificar duplicados\n    duplicates = df.duplicated(subset=['id']).sum()\n    if duplicates > 0:\n        print(f'Warning: Found {duplicates} duplicate IDs - keeping first occurrence')\n        df = df.drop_duplicates(subset=['id'])\n    \n    print(f'After validation: {len(df)} records to process')\n    \n    # Convertir columnas de timestamp a formato correcto\n    timestamp_columns = [\n        'ingested_at_utc', \n        'extract_window_start_utc', \n        'extract_window_end_utc'\n    ]\n    \n    for col in timestamp_columns:\n        if col in df.columns:\n            # Si la columna contiene strings de template sin resolver, usar datetime actual\n            if df[col].astype(str).str.contains('{{').any():\n                if col == 'ingested_at_utc':\n                    df[col] = pd.Timestamp.now(tz='UTC')\n                else:\n                    # Para extract_window, usar valores de los par\u00e1metros\n                    fecha_inicio = kwargs.get('fecha_inicio', '2025-08-14')\n                    fecha_fin = kwargs.get('fecha_fin', '2025-08-16')\n                    if col == 'extract_window_start_utc':\n                        df[col] = pd.Timestamp(fecha_inicio, tz='UTC')\n                    else:  # extract_window_end_utc\n                        df[col] = pd.Timestamp(fecha_fin, tz='UTC')\n            else:\n                # Convertir a timestamp si no es ya timestamp\n                df[col] = pd.to_datetime(df[col], utc=True)\n    \n    # Asegurar tipos de datos correctos\n    if 'page_number' in df.columns:\n        df['page_number'] = df['page_number'].astype(int)\n    if 'page_size' in df.columns:\n        df['page_size'] = df['page_size'].astype(int)\n    \n    # Asegurar que payload y request_payload sean strings JSON v\u00e1lidos\n    if 'payload' in df.columns:\n        df['payload'] = df['payload'].astype(str)\n    if 'request_payload' in df.columns:\n        df['request_payload'] = df['request_payload'].astype(str)\n    \n    print('Data preprocessing: Done')\n    \n    # Implementar idempotencia con UPSERT manual\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        \n        # Verificar registros existentes para evitar duplicados\n        if len(df) > 0:\n            ids_to_check = \"','\".join(df['id'].astype(str).tolist())\n            existing_query = f\"\"\"\n                SELECT id FROM {schema_name}.{table_name} \n                WHERE id IN ('{ids_to_check}')\n            \"\"\"\n            \n            try:\n                existing_records = loader.load(existing_query)\n                existing_ids = set(existing_records['id'].tolist()) if len(existing_records) > 0 else set()\n                \n                # Filtrar solo registros nuevos\n                new_records_df = df[~df['id'].isin(existing_ids)]\n                \n                if len(new_records_df) > 0:\n                    print(f'Inserting {len(new_records_df)} new records (skipping {len(existing_ids)} existing)')\n                    loader.export(\n                        new_records_df,\n                        schema_name,\n                        table_name,\n                        index=False,\n                        if_exists='append',\n                        allow_reserved_words=True\n                    )\n                else:\n                    print('All records already exist - idempotent operation complete')\n                    \n            except Exception as e:\n                print(f'Warning during idempotence check: {e}')\n                print('Proceeding with regular append...')\n                loader.export(\n                    df,\n                    schema_name,\n                    table_name,\n                    index=False,\n                    if_exists='append',\n                    allow_reserved_words=True\n                )\n    \n    print('Data export to PostgreSQL: Done')\n    \n    # Verificar datos insertados y generar m\u00e9tricas\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        # Conteo total\n        count_query = f\"SELECT COUNT(*) as total FROM {schema_name}.{table_name}\"\n        result = loader.load(count_query)\n        total_records = result.iloc[0]['total']\n        \n        # M\u00e9tricas adicionales\n        metrics_query = f\"\"\"\n            SELECT \n                COUNT(*) as total_records,\n                COUNT(DISTINCT id) as unique_ids,\n                MIN(ingested_at_utc) as first_ingested,\n                MAX(ingested_at_utc) as last_ingested\n            FROM {schema_name}.{table_name}\n        \"\"\"\n        metrics = loader.load(metrics_query)\n        \n        print(f'=== ITEMS PROCESSING METRICS ===')\n        print(f'Total records in table: {total_records}')\n        print(f'Unique IDs: {metrics.iloc[0][\"unique_ids\"]}')\n        print(f'Records processed this run: {len(df)}')\n        print(f'First ingested: {metrics.iloc[0][\"first_ingested\"]}')\n        print(f'Last ingested: {metrics.iloc[0][\"last_ingested\"]}')\n        print(f'================================')\n    \n    print(f'Successfully processed {len(df)} items records to {schema_name}.{table_name}')", "file_path": "data_exporters/save_to_raw_items_py.py", "language": "python", "type": "data_exporter", "uuid": "save_to_raw_items_py"}, "data_exporters/save_to_raw_customers_py.py:data_exporter:python:save to raw customers py": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\nimport pandas as pd\nfrom datetime import datetime, timezone\n\n@data_exporter\ndef export_customers_to_raw(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Data exporter para guardar clientes de QuickBooks en PostgreSQL raw\n    Implementa idempotencia, validaciones de calidad y m\u00e9tricas detalladas\n    \"\"\"\n    \n    print(\"=== QB CUSTOMERS DATA EXPORT STARTED ===\")\n    \n    # Configuraci\u00f3n de base de datos\n    schema_name = 'raw'\n    table_name = 'qb_customers'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    \n    print(f\"Target table: {schema_name}.{table_name}\")\n    print(f\"Input records: {len(df)}\")\n    \n    # Validaciones de calidad de datos\n    if len(df) == 0:\n        print(\"WARNING: No records to process - export aborted\")\n        return\n    \n    print(\"--- DATA QUALITY VALIDATIONS ---\")\n    \n    # 1. Verificar IDs no nulos\n    initial_count = len(df)\n    df = df[df['id'].notnull()]\n    null_ids_removed = initial_count - len(df)\n    if null_ids_removed > 0:\n        print(f\"VALIDATION: Removed {null_ids_removed} records with null IDs\")\n    \n    # 2. Verificar y remover duplicados\n    initial_count = len(df)\n    df = df.drop_duplicates(subset=['id'], keep='first')\n    duplicates_removed = initial_count - len(df)\n    if duplicates_removed > 0:\n        print(f\"VALIDATION: Removed {duplicates_removed} duplicate IDs\")\n    \n    # 3. Validar formato de IDs\n    invalid_ids = df[df['id'].astype(str).str.len() == 0]\n    if len(invalid_ids) > 0:\n        print(f\"VALIDATION: Found {len(invalid_ids)} records with empty IDs\")\n        df = df[df['id'].astype(str).str.len() > 0]\n    \n    print(f\"Records after validation: {len(df)}\")\n    \n    if len(df) == 0:\n        print(\"ERROR: No valid records remain after validation\")\n        return\n    \n    # Preparaci\u00f3n de datos para PostgreSQL\n    print(\"--- DATA PREPROCESSING ---\")\n    \n    # Obtener par\u00e1metros de runtime\n    fecha_inicio = kwargs.get('fecha_inicio', '2025-01-01')\n    fecha_fin = kwargs.get('fecha_fin', '2025-09-16')\n    \n    # Procesar columnas de timestamp\n    timestamp_columns = [\n        'ingested_at_utc', \n        'extract_window_start_utc', \n        'extract_window_end_utc'\n    ]\n    \n    current_utc = datetime.now(timezone.utc)\n    \n    for col in timestamp_columns:\n        if col in df.columns:\n            if col == 'ingested_at_utc':\n                df[col] = current_utc\n            elif col == 'extract_window_start_utc':\n                df[col] = pd.Timestamp(fecha_inicio, tz='UTC')\n            elif col == 'extract_window_end_utc':\n                df[col] = pd.Timestamp(fecha_fin, tz='UTC')\n            else:\n                # Para cualquier otra columna timestamp, convertir apropiadamente\n                df[col] = pd.to_datetime(df[col], utc=True)\n    \n    # Asegurar tipos de datos correctos\n    numeric_columns = ['page_number', 'page_size']\n    for col in numeric_columns:\n        if col in df.columns:\n            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n    \n    # Asegurar que JSON fields sean strings v\u00e1lidos\n    json_columns = ['payload', 'request_payload']\n    for col in json_columns:\n        if col in df.columns:\n            df[col] = df[col].astype(str)\n    \n    print(\"Data preprocessing completed\")\n    \n    # Implementar idempotencia con verificaci\u00f3n de registros existentes\n    print(\"--- IDEMPOTENCE CHECK ---\")\n    \n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        \n        try:\n            # Verificar registros existentes\n            existing_ids_query = f\"\"\"\n                SELECT DISTINCT id \n                FROM {schema_name}.{table_name} \n                WHERE id = ANY(ARRAY[{','.join([\"'\" + str(id_val) + \"'\" for id_val in df['id'].tolist()])}])\n            \"\"\"\n            \n            existing_records = loader.load(existing_ids_query)\n            existing_ids_set = set(existing_records['id'].tolist()) if len(existing_records) > 0 else set()\n            \n            # Filtrar solo registros nuevos\n            new_records_df = df[~df['id'].isin(existing_ids_set)]\n            existing_count = len(df) - len(new_records_df)\n            \n            print(f\"Existing records found: {existing_count}\")\n            print(f\"New records to insert: {len(new_records_df)}\")\n            \n            if len(new_records_df) > 0:\n                print(\"--- DATABASE EXPORT ---\")\n                \n                # Insertar solo registros nuevos\n                loader.export(\n                    new_records_df,\n                    schema_name,\n                    table_name,\n                    index=False,\n                    if_exists='append',\n                    allow_reserved_words=True\n                )\n                \n                print(f\"Successfully inserted {len(new_records_df)} new records\")\n                \n            else:\n                print(\"IDEMPOTENCE: All records already exist - no inserts needed\")\n            \n        except Exception as e:\n            print(f\"ERROR during idempotence check: {e}\")\n            print(\"Falling back to standard append operation...\")\n            \n            # Fallback: insertar todo y dejar que la DB maneje duplicados\n            loader.export(\n                df,\n                schema_name,\n                table_name,\n                index=False,\n                if_exists='append',\n                allow_reserved_words=True\n            )\n            \n            print(f\"Fallback insert completed for {len(df)} records\")\n    \n    # Verificaci\u00f3n final y m\u00e9tricas\n    print(\"--- FINAL VERIFICATION AND METRICS ---\")\n    \n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        \n        # M\u00e9tricas generales de la tabla\n        metrics_query = f\"\"\"\n            SELECT \n                COUNT(*) as total_records,\n                COUNT(DISTINCT id) as unique_customers,\n                MIN(ingested_at_utc) as first_ingested,\n                MAX(ingested_at_utc) as last_ingested,\n                COUNT(*) FILTER (WHERE DATE(ingested_at_utc) = CURRENT_DATE) as records_today\n            FROM {schema_name}.{table_name}\n        \"\"\"\n        \n        metrics_result = loader.load(metrics_query)\n        metrics = metrics_result.iloc[0]\n        \n        # Verificaci\u00f3n de integridad\n        integrity_query = f\"\"\"\n            SELECT \n                COUNT(*) FILTER (WHERE id IS NULL) as null_ids,\n                COUNT(*) - COUNT(DISTINCT id) as duplicate_ids,\n                COUNT(*) FILTER (WHERE payload IS NULL) as null_payloads\n            FROM {schema_name}.{table_name}\n        \"\"\"\n        \n        integrity_result = loader.load(integrity_query)\n        integrity = integrity_result.iloc[0]\n        \n        # M\u00e9tricas por ventana de extracci\u00f3n\n        window_query = f\"\"\"\n            SELECT \n                extract_window_start_utc,\n                extract_window_end_utc,\n                COUNT(*) as records_in_window\n            FROM {schema_name}.{table_name}\n            WHERE extract_window_start_utc = '{fecha_inicio}'\n              AND extract_window_end_utc = '{fecha_fin}'\n            GROUP BY extract_window_start_utc, extract_window_end_utc\n        \"\"\"\n        \n        window_result = loader.load(window_query)\n        \n        # Imprimir m\u00e9tricas finales\n        print(\"=== EXPORT COMPLETION METRICS ===\")\n        print(f\"Total customers in table: {metrics['total_records']}\")\n        print(f\"Unique customer IDs: {metrics['unique_customers']}\")\n        print(f\"Records processed this run: {len(df)}\")\n        print(f\"Records ingested today: {metrics['records_today']}\")\n        print(f\"First record ever: {metrics['first_ingested']}\")\n        print(f\"Last record: {metrics['last_ingested']}\")\n        \n        print(\"=== DATA INTEGRITY VERIFICATION ===\")\n        print(f\"Null IDs in table: {integrity['null_ids']}\")\n        print(f\"Duplicate IDs in table: {integrity['duplicate_ids']}\")\n        print(f\"Null payloads in table: {integrity['null_payloads']}\")\n        \n        if len(window_result) > 0:\n            window_records = window_result.iloc[0]['records_in_window']\n            print(f\"Records for window {fecha_inicio} to {fecha_fin}: {window_records}\")\n        \n        # Verificar idempotencia\n        if integrity['duplicate_ids'] == 0 and integrity['null_ids'] == 0:\n            print(\"INTEGRITY CHECK: PASSED - No duplicates or null IDs found\")\n        else:\n            print(\"INTEGRITY CHECK: FAILED - Data quality issues detected\")\n    \n    print(\"=== QB CUSTOMERS DATA EXPORT COMPLETED ===\")\n    \n    # Log de resumen final\n    summary = {\n        'input_records': len(df),\n        'target_table': f\"{schema_name}.{table_name}\",\n        'date_range': f\"{fecha_inicio} to {fecha_fin}\",\n        'timestamp': current_utc.isoformat()\n    }\n    \n    print(f\"Export summary: {summary}\")\n    \n    return None", "file_path": "data_exporters/save_to_raw_customers_py.py", "language": "python", "type": "data_exporter", "uuid": "save_to_raw_customers_py"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/extract_qb_invoices.py:data_loader:python:extract qb invoices": {"content": "import requests\nimport json\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\n@data_loader\ndef extract_qb_customers(*args, **kwargs):\n    \"\"\"\n    Data loader para extraer clientes de QuickBooks Online usando API real\n    \"\"\"\n    \n    # Obtener secrets reales\n    client_id = get_secret_value('QB_CLIENT_ID')\n    client_secret = get_secret_value('QB_CLIENT_SECRET')\n    realm_id = get_secret_value('QB_REALM_ID')\n    refresh_token = get_secret_value('QB_REFRESH_TOKEN')\n    environment = get_secret_value('QB_ENVIRONMENT')\n    \n    # Par\u00e1metros del pipeline desde el trigger\n    fecha_inicio = kwargs.get('fecha_inicio', '2025-08-14')\n    fecha_fin = kwargs.get('fecha_fin', '2025-08-16')\n    \n    print('Parsing of secrets: Done')\n    \n    # Obtener access token real\n    access_token = get_access_token(client_id, client_secret, refresh_token)\n    print('OAuth authentication: Done')\n    \n    # URL base de QuickBooks\n    if environment == 'sandbox':\n        base_url = f\"https://sandbox-quickbooks.api.intuit.com/v3/company/{realm_id}\"\n    else:\n        base_url = f\"https://quickbooks.api.intuit.com/v3/company/{realm_id}\"\n    \n    # Headers para API\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json'\n    }\n    \n    # Extraer datos reales de QuickBooks API (sin filtros por limitaciones de API)\n    all_customers = []\n    start_position = 1\n    page_size = 100\n    \n    while True:\n        # Query para customers\n        query = \"SELECT * FROM Customer\"\n        \n        params = {\n            'query': query,\n            'startPosition': start_position,\n            'maxResults': page_size\n        }\n        \n        try:\n            print(f'Downloading customers from position {start_position}...')\n            \n            response = requests.get(f\"{base_url}/query\", headers=headers, params=params)\n            response.raise_for_status()\n            \n            data = response.json()\n            customers = data.get('QueryResponse', {}).get('Customer', [])\n            \n            if not customers:\n                print('No more customers to download')\n                break\n            \n            all_customers.extend(customers)\n            start_position += page_size\n            \n            print(f'Downloaded {len(customers)} customers')\n            \n            # Si obtenemos menos de page_size, ya no hay m\u00e1s datos\n            if len(customers) < page_size:\n                break\n                \n        except requests.exceptions.RequestException as e:\n            print(f'API Error: {e}')\n            if hasattr(e, 'response') and e.response:\n                print(f'Response content: {e.response.text}')\n            break\n    \n    print(f'Total customers extracted from API: {len(all_customers)}')\n    \n    # Aplicar filtro hist\u00f3rico del lado del cliente\n    filtered_customers = []\n    for customer in all_customers:\n        last_updated = customer.get('MetaData', {}).get('LastUpdatedTime', '')\n        if last_updated:\n            # Extraer solo la fecha (YYYY-MM-DD) para comparar\n            customer_date = last_updated[:10]  # \"2025-08-15T13:31:45-07:00\" -> \"2025-08-15\"\n            if fecha_inicio <= customer_date <= fecha_fin:\n                filtered_customers.append(customer)\n    \n    all_customers = filtered_customers\n    print(f'Customers after date filtering ({fecha_inicio} to {fecha_fin}): {len(all_customers)}')\n    \n    # Procesar datos para PostgreSQL\n    processed_records = []\n    current_time = datetime.now(timezone.utc)\n    \n    for customer in all_customers:\n        processed_record = {\n            'id': customer.get('Id'),\n            'payload': json.dumps(customer),  # JSON completo\n            'ingested_at_utc': current_time,\n            'extract_window_start_utc': fecha_inicio,\n            'extract_window_end_utc': fecha_fin,\n            'page_number': 1,\n            'page_size': len(all_customers),\n            'request_payload': json.dumps({\n                'entity_type': 'customers',\n                'fecha_inicio': fecha_inicio,\n                'fecha_fin': fecha_fin,\n                'environment': environment,\n                'realm_id': realm_id,\n                'client_side_filtering': True\n            })\n        }\n        processed_records.append(processed_record)\n    \n    # Retornar DataFrame\n    df = pd.DataFrame(processed_records)\n    \n    print(f'Data processing: Done - {len(df)} records ready for export')\n    \n    return df\n\n\ndef get_access_token(client_id, client_secret, refresh_token):\n    \"\"\"\n    Obtener access token real de QuickBooks OAuth 2.0\n    \"\"\"\n    import base64\n    \n    print('Getting fresh access token...')\n    \n    token_url = 'https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer'\n    \n    credentials = f\"{client_id}:{client_secret}\"\n    encoded_credentials = base64.b64encode(credentials.encode()).decode()\n    \n    headers = {\n        'Authorization': f'Basic {encoded_credentials}',\n        'Content-Type': 'application/x-www-form-urlencoded'\n    }\n    \n    data = {\n        'grant_type': 'refresh_token',\n        'refresh_token': refresh_token\n    }\n    \n    response = requests.post(token_url, headers=headers, data=data)\n    \n    if response.status_code == 200:\n        token_data = response.json()\n        print('Access token obtained successfully')\n        return token_data['access_token']\n    else:\n        print(f'Token error: {response.status_code}')\n        print(f'Error response: {response.text}')\n        raise Exception(f\"Error getting access token: {response.text}\")", "file_path": "data_loaders/extract_qb_invoices.py", "language": "python", "type": "data_loader", "uuid": "extract_qb_invoices"}, "data_loaders/extract_qb_items_py.py:data_loader:python:extract qb items py": {"content": "import requests\nimport json\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\n@data_loader\ndef extract_qb_customers(*args, **kwargs):\n    \"\"\"\n    Data loader CLIENTES\n    \"\"\"\n    \n    # Obtener secrets \n    client_id = get_secret_value('QB_CLIENT_ID')\n    client_secret = get_secret_value('QB_CLIENT_SECRET')\n    realm_id = get_secret_value('QB_REALM_ID')\n    refresh_token = get_secret_value('QB_REFRESH_TOKEN')\n    environment = get_secret_value('QB_ENVIRONMENT')\n    \n    \n    fecha_inicio = kwargs.get('fecha_inicio', '2025-08-14')\n    fecha_fin = kwargs.get('fecha_fin', '2025-08-16')\n    \n    print('Parsing of secrets: Done')\n    \n    # access token real\n    access_token = get_access_token(client_id, client_secret, refresh_token)\n    print('OAuth authentication: Done')\n    \n    # URL\n    if environment == 'sandbox':\n        base_url = f\"https://sandbox-quickbooks.api.intuit.com/v3/company/{realm_id}\"\n    else:\n        base_url = f\"https://quickbooks.api.intuit.com/v3/company/{realm_id}\"\n    \n    # Headers \n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json'\n    }\n    \n    # Extraer\n    all_customers = []\n    start_position = 1\n    page_size = 100\n    \n    while True:\n        # Query para customers\n        query = \"SELECT * FROM Customer\"\n        \n        params = {\n            'query': query,\n            'startPosition': start_position,\n            'maxResults': page_size\n        }\n        \n        try:\n            print(f'Downloading customers from position {start_position}...')\n            \n            response = requests.get(f\"{base_url}/query\", headers=headers, params=params)\n            response.raise_for_status()\n            \n            data = response.json()\n            customers = data.get('QueryResponse', {}).get('Customer', [])\n            \n            if not customers:\n                print('No more customers to download')\n                break\n            \n            all_customers.extend(customers)\n            start_position += page_size\n            \n            print(f'Downloaded {len(customers)} customers')\n            \n            # Si obtenemos menos de page_size, ya no hay m\u00e1s datos\n            if len(customers) < page_size:\n                break\n                \n        except requests.exceptions.RequestException as e:\n            print(f'API Error: {e}')\n            if hasattr(e, 'response') and e.response:\n                print(f'Response content: {e.response.text}')\n            break\n    \n    print(f'Total customers extracted from API: {len(all_customers)}')\n    \n    # Aplicar filtro hist\u00f3rico del lado del cliente\n    filtered_customers = []\n    for customer in all_customers:\n        last_updated = customer.get('MetaData', {}).get('LastUpdatedTime', '')\n        if last_updated:\n            # Extraer solo la fecha (YYYY-MM-DD) para comparar\n            customer_date = last_updated[:10]  # \"2025-08-15T13:31:45-07:00\" -> \"2025-08-15\"\n            if fecha_inicio <= customer_date <= fecha_fin:\n                filtered_customers.append(customer)\n    \n    all_customers = filtered_customers\n    print(f'Customers after date filtering ({fecha_inicio} to {fecha_fin}): {len(all_customers)}')\n    \n    # Procesar datos para PostgreSQL\n    processed_records = []\n    current_time = datetime.now(timezone.utc)\n    \n    for customer in all_customers:\n        processed_record = {\n            'id': customer.get('Id'),\n            'payload': json.dumps(customer),  # JSON completo\n            'ingested_at_utc': current_time,\n            'extract_window_start_utc': fecha_inicio,\n            'extract_window_end_utc': fecha_fin,\n            'page_number': 1,\n            'page_size': len(all_customers),\n            'request_payload': json.dumps({\n                'entity_type': 'customers',\n                'fecha_inicio': fecha_inicio,\n                'fecha_fin': fecha_fin,\n                'environment': environment,\n                'realm_id': realm_id,\n                'client_side_filtering': True\n            })\n        }\n        processed_records.append(processed_record)\n    \n    # Retornar DataFrame\n    df = pd.DataFrame(processed_records)\n    \n    print(f'Data processing: Done - {len(df)} records ready for export')\n    \n    return df\n\n\ndef get_access_token(client_id, client_secret, refresh_token):\n    \"\"\"\n    Obtener access token real de QuickBooks OAuth 2.0\n    \"\"\"\n    import base64\n    \n    print('Getting fresh access token...')\n    \n    token_url = 'https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer'\n    \n    credentials = f\"{client_id}:{client_secret}\"\n    encoded_credentials = base64.b64encode(credentials.encode()).decode()\n    \n    headers = {\n        'Authorization': f'Basic {encoded_credentials}',\n        'Content-Type': 'application/x-www-form-urlencoded'\n    }\n    \n    data = {\n        'grant_type': 'refresh_token',\n        'refresh_token': refresh_token\n    }\n    \n    response = requests.post(token_url, headers=headers, data=data)\n    \n    if response.status_code == 200:\n        token_data = response.json()\n        print('Access token obtained successfully')\n        return token_data['access_token']\n    else:\n        print(f'Token error: {response.status_code}')\n        print(f'Error response: {response.text}')\n        raise Exception(f\"Error getting access token: {response.text}\")", "file_path": "data_loaders/extract_qb_items_py.py", "language": "python", "type": "data_loader", "uuid": "extract_qb_items_py"}, "data_loaders/extract_qb_customers_py.py:data_loader:python:extract qb customers py": {"content": "import requests\nimport json\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport time\n\n@data_loader\ndef extract_qb_customers(*args, **kwargs):\n    \"\"\"\n    Data loader para extraer clientes de QuickBooks Online usando API real\n    Implementa paginaci\u00f3n, rate limits, filtros hist\u00f3ricos e idempotencia\n    \"\"\"\n    \n    print(\"=== QUICKBOOKS CUSTOMERS BACKFILL STARTED ===\")\n    \n    # Obtener secrets desde Mage Secrets\n    try:\n        client_id = get_secret_value('QB_CLIENT_ID')\n        client_secret = get_secret_value('QB_CLIENT_SECRET')\n        realm_id = get_secret_value('QB_REALM_ID')\n        refresh_token = get_secret_value('QB_REFRESH_TOKEN')\n        environment = get_secret_value('QB_ENVIRONMENT')\n        print(\"\u2713 Secrets loaded successfully\")\n    except Exception as e:\n        raise Exception(f\"Error loading secrets: {e}\")\n    \n    # Par\u00e1metros del pipeline desde runtime variables\n    fecha_inicio = kwargs.get('fecha_inicio', '2025-01-01')\n    fecha_fin = kwargs.get('fecha_fin', '2025-09-16')\n    \n    print(f\"\ud83d\udcc5 Date range: {fecha_inicio} to {fecha_fin}\")\n    \n    # Obtener access token usando OAuth2\n    access_token = get_fresh_access_token(client_id, client_secret, refresh_token)\n    print(\"\ud83d\udd10 OAuth2 authentication successful\")\n    \n    # Configurar URL base seg\u00fan environment\n    if environment.lower() == 'sandbox':\n        base_url = f\"https://sandbox-quickbooks.api.intuit.com/v3/company/{realm_id}\"\n    else:\n        base_url = f\"https://quickbooks.api.intuit.com/v3/company/{realm_id}\"\n    \n    print(f\"\ud83c\udf10 Using {environment} environment\")\n    \n    # Headers para todas las requests\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'User-Agent': 'MageAI-QB-Pipeline/1.0'\n    }\n    \n    # Extraer todos los customers con paginaci\u00f3n\n    all_customers = []\n    start_position = 1\n    page_size = 100\n    total_requests = 0\n    \n    print(\"\ud83d\udce1 Starting data extraction from QuickBooks API...\")\n    \n    while True:\n        # Query SQL para QuickBooks API\n        query = \"SELECT * FROM Customer\"\n        \n        params = {\n            'query': query,\n            'startPosition': start_position,\n            'maxResults': page_size\n        }\n        \n        try:\n            print(f\"   \ud83d\udcc4 Page {(start_position-1)//page_size + 1}: fetching records {start_position}-{start_position + page_size - 1}\")\n            \n            # Request con manejo de rate limits\n            response = make_api_request(f\"{base_url}/query\", headers, params)\n            total_requests += 1\n            \n            data = response.json()\n            customers = data.get('QueryResponse', {}).get('Customer', [])\n            \n            if not customers:\n                print(\"   \u2713 No more customers found - pagination complete\")\n                break\n            \n            all_customers.extend(customers)\n            start_position += page_size\n            \n            print(f\"   \u2713 Retrieved {len(customers)} customers\")\n            \n            # Si obtenemos menos registros del page_size, llegamos al final\n            if len(customers) < page_size:\n                print(\"   \u2713 Last page reached\")\n                break\n                \n            # Pausa breve para evitar rate limits\n            time.sleep(0.3)\n                \n        except Exception as e:\n            print(f\"\u274c API Error on page {(start_position-1)//page_size + 1}: {e}\")\n            break\n    \n    print(f\"\ud83d\udcca Total API requests made: {total_requests}\")\n    print(f\"\ud83d\udcca Total customers extracted: {len(all_customers)}\")\n    \n    # Aplicar filtro hist\u00f3rico por fecha (client-side filtering)\n    print(f\"\ud83d\udd0d Applying date filter: {fecha_inicio} to {fecha_fin}\")\n    \n    filtered_customers = []\n    for customer in all_customers:\n        # Usar MetaData.LastUpdatedTime para filtrar\n        last_updated = customer.get('MetaData', {}).get('LastUpdatedTime', '')\n        if last_updated:\n            # Extraer solo la fecha YYYY-MM-DD\n            customer_date = last_updated[:10]\n            if fecha_inicio <= customer_date <= fecha_fin:\n                filtered_customers.append(customer)\n    \n    print(f\"\ud83d\udcca Customers after date filtering: {len(filtered_customers)}\")\n    \n    # Procesar datos para el formato de la tabla raw\n    processed_records = []\n    current_time = datetime.now(timezone.utc)\n    \n    print(\"\ud83d\udd04 Processing records for database...\")\n    \n    for customer in filtered_customers:\n        processed_record = {\n            'id': customer.get('Id'),\n            'payload': json.dumps(customer, ensure_ascii=False),  # JSON completo\n            'ingested_at_utc': current_time,\n            'extract_window_start_utc': fecha_inicio,\n            'extract_window_end_utc': fecha_fin,\n            'page_number': (len(processed_records) // page_size) + 1,\n            'page_size': page_size,\n            'request_payload': json.dumps({\n                'entity_type': 'customers',\n                'fecha_inicio': fecha_inicio,\n                'fecha_fin': fecha_fin,\n                'environment': environment,\n                'realm_id': realm_id,\n                'total_api_requests': total_requests,\n                'client_side_filtering': True,\n                'extraction_timestamp': current_time.isoformat()\n            }, ensure_ascii=False)\n        }\n        processed_records.append(processed_record)\n    \n    # Crear DataFrame final\n    df = pd.DataFrame(processed_records)\n    \n    print(f\"\u2705 Data processing complete: {len(df)} records ready for export\")\n    print(\"=== QUICKBOOKS CUSTOMERS EXTRACTION FINISHED ===\")\n    \n    return df\n\n\ndef get_fresh_access_token(client_id, client_secret, refresh_token):\n    \"\"\"\n    Obtener nuevo access token usando refresh token\n    Implementa reintentos con backoff exponencial\n    \"\"\"\n    import base64\n    \n    token_url = 'https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer'\n    \n    # Codificar credenciales\n    credentials = f\"{client_id}:{client_secret}\"\n    encoded_credentials = base64.b64encode(credentials.encode()).decode()\n    \n    headers = {\n        'Authorization': f'Basic {encoded_credentials}',\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Accept': 'application/json'\n    }\n    \n    data = {\n        'grant_type': 'refresh_token',\n        'refresh_token': refresh_token\n    }\n    \n    max_retries = 3\n    for attempt in range(max_retries):\n        try:\n            response = requests.post(token_url, headers=headers, data=data, timeout=30)\n            \n            if response.status_code == 200:\n                token_data = response.json()\n                return token_data['access_token']\n            else:\n                error_msg = f\"Token request failed: {response.status_code} - {response.text}\"\n                if attempt < max_retries - 1:\n                    print(f\"\u26a0\ufe0f  {error_msg}. Retrying... ({attempt + 1}/{max_retries})\")\n                    time.sleep(2 ** attempt)  # Backoff exponencial\n                    continue\n                else:\n                    raise Exception(error_msg)\n                    \n        except requests.exceptions.RequestException as e:\n            if attempt < max_retries - 1:\n                print(f\"\u26a0\ufe0f  Token request error: {e}. Retrying... ({attempt + 1}/{max_retries})\")\n                time.sleep(2 ** attempt)\n                continue\n            else:\n                raise Exception(f\"Failed to get access token after {max_retries} attempts: {e}\")\n\n\ndef make_api_request(url, headers, params, max_retries=3):\n    \"\"\"\n    Hacer request a la API con manejo de rate limits y reintentos\n    \"\"\"\n    for attempt in range(max_retries):\n        try:\n            response = requests.get(url, headers=headers, params=params, timeout=60)\n            \n            # Manejo espec\u00edfico de rate limits\n            if response.status_code == 429:\n                retry_after = int(response.headers.get('Retry-After', 60))\n                print(f\"   \u23f3 Rate limit hit. Waiting {retry_after} seconds...\")\n                time.sleep(retry_after)\n                continue\n            \n            # Verificar otros errores HTTP\n            response.raise_for_status()\n            return response\n            \n        except requests.exceptions.Timeout:\n            if attempt < max_retries - 1:\n                print(f\"   \u23f3 Request timeout. Retrying... ({attempt + 1}/{max_retries})\")\n                time.sleep(2 ** attempt)\n                continue\n            else:\n                raise Exception(\"Request timed out after multiple attempts\")\n                \n        except requests.exceptions.RequestException as e:\n            if attempt < max_retries - 1:\n                print(f\"   \u26a0\ufe0f  Request error: {e}. Retrying... ({attempt + 1}/{max_retries})\")\n                time.sleep(2 ** attempt)\n                continue\n            else:\n                raise Exception(f\"API request failed after {max_retries} attempts: {e}\")\n    \n    raise Exception(\"API request failed after all retry attempts\")", "file_path": "data_loaders/extract_qb_customers_py.py", "language": "python", "type": "data_loader", "uuid": "extract_qb_customers_py"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "pipelines/example_pipeline/__init__.py:pipeline:python:example pipeline/  init  ": {"content": "", "file_path": "pipelines/example_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "example_pipeline/__init__"}, "pipelines/example_pipeline/metadata.yaml:pipeline:yaml:example pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - fill_in_missing_values\n  name: load_titanic\n  status: not_executed\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_titanic\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - export_titanic_clean\n  name: fill_in_missing_values\n  status: not_executed\n  type: transformer\n  upstream_blocks:\n  - load_titanic\n  uuid: fill_in_missing_values\n- all_upstream_blocks_executed: true\n  downstream_blocks: []\n  name: export_titanic_clean\n  status: not_executed\n  type: data_exporter\n  upstream_blocks:\n  - fill_in_missing_values\n  uuid: export_titanic_clean\nname: example_pipeline\ntype: python\nuuid: example_pipeline\nwidgets: []\n", "file_path": "pipelines/example_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "example_pipeline/metadata"}, "pipelines/qb_invoices_backfill/__init__.py:pipeline:python:qb invoices backfill/  init  ": {"content": "", "file_path": "pipelines/qb_invoices_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_invoices_backfill/__init__"}, "pipelines/qb_invoices_backfill/metadata.yaml:pipeline:yaml:qb invoices backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - save_to_raw_invoices\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: extract_qb_invoices\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: extract_qb_invoices\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: save_to_raw_invoices\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - extract_qb_invoices\n  uuid: save_to_raw_invoices\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-11 21:45:53.512399+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_invoices_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_invoices_backfill\nvariables:\n  fecha_fin: '2025-12-31'\n  fecha_inicio: '2024-01-01'\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/qb_invoices_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_invoices_backfill/metadata"}, "pipelines/qb_items_backfill/__init__.py:pipeline:python:qb items backfill/  init  ": {"content": "", "file_path": "pipelines/qb_items_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_items_backfill/__init__"}, "pipelines/qb_items_backfill/metadata.yaml:pipeline:yaml:qb items backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - save_to_raw_items_py\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: extract_qb_items.py\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: extract_qb_items_py\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: save_to_raw_items.py\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - extract_qb_items_py\n  uuid: save_to_raw_items_py\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-11 21:45:53.512399+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_items_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_items_backfill\nvariables:\n  fecha_fin: '2025-12-31'\n  fecha_inicio: '2024-01-01'\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/qb_items_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_items_backfill/metadata"}, "pipelines/qb_customers_backfill/__init__.py:pipeline:python:qb customers backfill/  init  ": {"content": "", "file_path": "pipelines/qb_customers_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_customers_backfill/__init__"}, "pipelines/qb_customers_backfill/metadata.yaml:pipeline:yaml:qb customers backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - save_to_raw_customers_py\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: extract_qb_customers.py\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: extract_qb_customers_py\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: save_to_raw_customers.py\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - extract_qb_customers_py\n  uuid: save_to_raw_customers_py\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-11 21:45:53.512399+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_customers_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_customers_backfill\nvariables:\n  fecha_fin: '2025-12-31'\n  fecha_inicio: '2024-01-01'\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/qb_customers_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_customers_backfill/metadata"}, "/home/src/scheduler/data_loaders/extract_qb_customers_py.py:data_loader:python:home/src/scheduler/data loaders/extract qb customers py": {"content": "import requests\nimport json\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport time\n\n@data_loader\ndef extract_qb_customers(*args, **kwargs):\n    client_id = get_secret_value('QB_CLIENT_ID')\n    client_secret = get_secret_value('QB_CLIENT_SECRET')\n    realm_id = get_secret_value('QB_REALM_ID')\n    refresh_token = get_secret_value('QB_REFRESH_TOKEN')\n    environment = get_secret_value('QB_ENVIRONMENT')\n    \n    fecha_inicio = kwargs.get('fecha_inicio', '2025-01-01')\n    fecha_fin = kwargs.get('fecha_fin', '2025-09-16')\n    \n    access_token = get_access_token(client_id, client_secret, refresh_token)\n    \n    if environment.lower() == 'sandbox':\n        base_url = f\"https://sandbox-quickbooks.api.intuit.com/v3/company/{realm_id}\"\n    else:\n        base_url = f\"https://quickbooks.api.intuit.com/v3/company/{realm_id}\"\n    \n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json'\n    }\n    \n    all_customers = []\n    start_position = 1\n    page_size = 100\n    \n    while True:\n        query = \"SELECT * FROM Customer\"\n        \n        params = {\n            'query': query,\n            'startPosition': start_position,\n            'maxResults': page_size\n        }\n        \n        response = requests.get(f\"{base_url}/query\", headers=headers, params=params)\n        response.raise_for_status()\n        \n        data = response.json()\n        customers = data.get('QueryResponse', {}).get('Customer', [])\n        \n        if not customers:\n            break\n        \n        all_customers.extend(customers)\n        start_position += page_size\n        \n        if len(customers) < page_size:\n            break\n            \n        time.sleep(0.5)\n    \n    filtered_customers = []\n    for customer in all_customers:\n        last_updated = customer.get('MetaData', {}).get('LastUpdatedTime', '')\n        if last_updated:\n            customer_date = last_updated[:10]\n            if fecha_inicio <= customer_date <= fecha_fin:\n                filtered_customers.append(customer)\n    \n    processed_records = []\n    current_time = datetime.now(timezone.utc)\n    \n    for customer in filtered_customers:\n        processed_record = {\n            'id': customer.get('Id'),\n            'payload': json.dumps(customer),\n            'ingested_at_utc': current_time,\n            'extract_window_start_utc': fecha_inicio,\n            'extract_window_end_utc': fecha_fin,\n            'page_number': 1,\n            'page_size': len(filtered_customers),\n            'request_payload': json.dumps({\n                'entity_type': 'customers',\n                'fecha_inicio': fecha_inicio,\n                'fecha_fin': fecha_fin,\n                'environment': environment\n            })\n        }\n        processed_records.append(processed_record)\n    \n    return pd.DataFrame(processed_records)\n\n\ndef get_access_token(client_id, client_secret, refresh_token):\n    import base64\n    \n    token_url = 'https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer'\n    \n    credentials = f\"{client_id}:{client_secret}\"\n    encoded_credentials = base64.b64encode(credentials.encode()).decode()\n    \n    headers = {\n        'Authorization': f'Basic {encoded_credentials}',\n        'Content-Type': 'application/x-www-form-urlencoded'\n    }\n    \n    data = {\n        'grant_type': 'refresh_token',\n        'refresh_token': refresh_token\n    }\n    \n    response = requests.post(token_url, headers=headers, data=data)\n    \n    if response.status_code == 200:\n        return response.json()['access_token']\n    else:\n        raise Exception(f\"Error getting token: {response.text}\")", "file_path": "/home/src/scheduler/data_loaders/extract_qb_customers_py.py", "language": "python", "type": "data_loader", "uuid": "extract_qb_customers_py"}, "/home/src/scheduler/data_loaders/extract_qb_invoices.py:data_loader:python:home/src/scheduler/data loaders/extract qb invoices": {"content": "import requests\nimport json\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport time\n\n@data_loader\ndef extract_qb_invoices(*args, **kwargs):\n    client_id = get_secret_value('QB_CLIENT_ID')\n    client_secret = get_secret_value('QB_CLIENT_SECRET')\n    realm_id = get_secret_value('QB_REALM_ID')\n    refresh_token = get_secret_value('QB_REFRESH_TOKEN')\n    environment = get_secret_value('QB_ENVIRONMENT')\n    \n    fecha_inicio = kwargs.get('fecha_inicio', '2025-01-01')\n    fecha_fin = kwargs.get('fecha_fin', '2025-09-16')\n    \n    access_token = get_access_token(client_id, client_secret, refresh_token)\n    \n    if environment.lower() == 'sandbox':\n        base_url = f\"https://sandbox-quickbooks.api.intuit.com/v3/company/{realm_id}\"\n    else:\n        base_url = f\"https://quickbooks.api.intuit.com/v3/company/{realm_id}\"\n    \n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json'\n    }\n    \n    all_invoices = []\n    start_position = 1\n    page_size = 100\n    \n    while True:\n        query = \"SELECT * FROM Invoice\"\n        \n        params = {\n            'query': query,\n            'startPosition': start_position,\n            'maxResults': page_size\n        }\n        \n        response = requests.get(f\"{base_url}/query\", headers=headers, params=params)\n        response.raise_for_status()\n        \n        data = response.json()\n        invoices = data.get('QueryResponse', {}).get('Invoice', [])\n        \n        if not invoices:\n            break\n        \n        all_invoices.extend(invoices)\n        start_position += page_size\n        \n        if len(invoices) < page_size:\n            break\n            \n        time.sleep(0.5)\n    \n    filtered_invoices = []\n    for invoice in all_invoices:\n        txn_date = invoice.get('TxnDate', '')\n        last_updated = invoice.get('MetaData', {}).get('LastUpdatedTime', '')\n        \n        filter_date = txn_date if txn_date else last_updated[:10] if last_updated else ''\n        \n        if filter_date and fecha_inicio <= filter_date <= fecha_fin:\n            filtered_invoices.append(invoice)\n    \n    processed_records = []\n    current_time = datetime.now(timezone.utc)\n    \n    for invoice in filtered_invoices:\n        processed_record = {\n            'id': invoice.get('Id'),\n            'payload': json.dumps(invoice),\n            'ingested_at_utc': current_time,\n            'extract_window_start_utc': fecha_inicio,\n            'extract_window_end_utc': fecha_fin,\n            'page_number': 1,\n            'page_size': len(filtered_invoices),\n            'request_payload': json.dumps({\n                'entity_type': 'invoices',\n                'fecha_inicio': fecha_inicio,\n                'fecha_fin': fecha_fin,\n                'environment': environment\n            })\n        }\n        processed_records.append(processed_record)\n    \n    return pd.DataFrame(processed_records)\n\n\ndef get_access_token(client_id, client_secret, refresh_token):\n    import base64\n    \n    token_url = 'https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer'\n    \n    credentials = f\"{client_id}:{client_secret}\"\n    encoded_credentials = base64.b64encode(credentials.encode()).decode()\n    \n    headers = {\n        'Authorization': f'Basic {encoded_credentials}',\n        'Content-Type': 'application/x-www-form-urlencoded'\n    }\n    \n    data = {\n        'grant_type': 'refresh_token',\n        'refresh_token': refresh_token\n    }\n    \n    response = requests.post(token_url, headers=headers, data=data)\n    \n    if response.status_code == 200:\n        return response.json()['access_token']\n    else:\n        raise Exception(f\"Error getting token: {response.text}\")", "file_path": "/home/src/scheduler/data_loaders/extract_qb_invoices.py", "language": "python", "type": "data_loader", "uuid": "extract_qb_invoices"}, "/home/src/scheduler/data_loaders/extract_qb_items_py.py:data_loader:python:home/src/scheduler/data loaders/extract qb items py": {"content": "import requests\nimport json\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport time\n\n@data_loader\ndef extract_qb_items(*args, **kwargs):\n    client_id = get_secret_value('QB_CLIENT_ID')\n    client_secret = get_secret_value('QB_CLIENT_SECRET')\n    realm_id = get_secret_value('QB_REALM_ID')\n    refresh_token = get_secret_value('QB_REFRESH_TOKEN')\n    environment = get_secret_value('QB_ENVIRONMENT')\n    \n    fecha_inicio = kwargs.get('fecha_inicio', '2025-01-01')\n    fecha_fin = kwargs.get('fecha_fin', '2025-09-16')\n    \n    access_token = get_access_token(client_id, client_secret, refresh_token)\n    \n    if environment.lower() == 'sandbox':\n        base_url = f\"https://sandbox-quickbooks.api.intuit.com/v3/company/{realm_id}\"\n    else:\n        base_url = f\"https://quickbooks.api.intuit.com/v3/company/{realm_id}\"\n    \n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json'\n    }\n    \n    all_items = []\n    start_position = 1\n    page_size = 100\n    \n    while True:\n        query = \"SELECT * FROM Item\"\n        \n        params = {\n            'query': query,\n            'startPosition': start_position,\n            'maxResults': page_size\n        }\n        \n        try:\n            response = requests.get(f\"{base_url}/query\", headers=headers, params=params)\n            \n            if response.status_code == 429:\n                time.sleep(60)\n                continue\n                \n            response.raise_for_status()\n            \n            data = response.json()\n            items = data.get('QueryResponse', {}).get('Item', [])\n            \n            if not items:\n                break\n            \n            all_items.extend(items)\n            start_position += page_size\n            \n            if len(items) < page_size:\n                break\n                \n            time.sleep(0.5)\n            \n        except requests.exceptions.RequestException as e:\n            print(f\"Error: {e}\")\n            break\n    \n    filtered_items = []\n    for item in all_items:\n        last_updated = item.get('MetaData', {}).get('LastUpdatedTime', '')\n        if last_updated:\n            item_date = last_updated[:10]\n            if fecha_inicio <= item_date <= fecha_fin:\n                filtered_items.append(item)\n    \n    processed_records = []\n    current_time = datetime.now(timezone.utc)\n    \n    for item in filtered_items:\n        processed_record = {\n            'id': item.get('Id'),\n            'payload': json.dumps(item),\n            'ingested_at_utc': current_time,\n            'extract_window_start_utc': fecha_inicio,\n            'extract_window_end_utc': fecha_fin,\n            'page_number': 1,\n            'page_size': len(filtered_items),\n            'request_payload': json.dumps({\n                'entity_type': 'items',\n                'fecha_inicio': fecha_inicio,\n                'fecha_fin': fecha_fin,\n                'environment': environment\n            })\n        }\n        processed_records.append(processed_record)\n    \n    return pd.DataFrame(processed_records)\n\n\ndef get_access_token(client_id, client_secret, refresh_token):\n    import base64\n    \n    token_url = 'https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer'\n    \n    credentials = f\"{client_id}:{client_secret}\"\n    encoded_credentials = base64.b64encode(credentials.encode()).decode()\n    \n    headers = {\n        'Authorization': f'Basic {encoded_credentials}',\n        'Content-Type': 'application/x-www-form-urlencoded'\n    }\n    \n    data = {\n        'grant_type': 'refresh_token',\n        'refresh_token': refresh_token\n    }\n    \n    response = requests.post(token_url, headers=headers, data=data)\n    \n    if response.status_code == 200:\n        return response.json()['access_token']\n    else:\n        raise Exception(f\"Error getting token: {response.text}\")", "file_path": "/home/src/scheduler/data_loaders/extract_qb_items_py.py", "language": "python", "type": "data_loader", "uuid": "extract_qb_items_py"}, "/home/src/scheduler/data_exporters/save_to_raw_customers_py.py:data_exporter:python:home/src/scheduler/data exporters/save to raw customers py": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\nimport pandas as pd\n\n@data_exporter\ndef save_to_raw_customers(df: DataFrame, **kwargs) -> None:\n    schema_name = 'raw'\n    table_name = 'qb_customers'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    \n    if len(df) == 0:\n        print(\"No records to process\")\n        return\n    \n    df = df[df['id'].notnull()]\n    df = df.drop_duplicates(subset=['id'])\n    \n    fecha_inicio = kwargs.get('fecha_inicio', '2025-01-01')\n    fecha_fin = kwargs.get('fecha_fin', '2025-09-16')\n    \n    timestamp_columns = ['ingested_at_utc', 'extract_window_start_utc', 'extract_window_end_utc']\n    \n    for col in timestamp_columns:\n        if col in df.columns:\n            if col == 'ingested_at_utc':\n                df[col] = pd.Timestamp.now(tz='UTC')\n            elif col == 'extract_window_start_utc':\n                df[col] = pd.Timestamp(fecha_inicio, tz='UTC')\n            elif col == 'extract_window_end_utc':\n                df[col] = pd.Timestamp(fecha_fin, tz='UTC')\n    \n    if 'page_number' in df.columns:\n        df['page_number'] = df['page_number'].astype(int)\n    if 'page_size' in df.columns:\n        df['page_size'] = df['page_size'].astype(int)\n    \n    if 'payload' in df.columns:\n        df['payload'] = df['payload'].astype(str)\n    if 'request_payload' in df.columns:\n        df['request_payload'] = df['request_payload'].astype(str)\n    \n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        try:\n            if len(df) > 0:\n                ids_str = \"','\".join(df['id'].astype(str).tolist())\n                existing_query = f\"SELECT id FROM {schema_name}.{table_name} WHERE id IN ('{ids_str}')\"\n                \n                existing_records = loader.load(existing_query)\n                existing_ids = set(existing_records['id'].tolist()) if len(existing_records) > 0 else set()\n                \n                new_records_df = df[~df['id'].isin(existing_ids)]\n                \n                if len(new_records_df) > 0:\n                    loader.export(new_records_df, schema_name, table_name, index=False, if_exists='append', allow_reserved_words=True)\n                    print(f\"Inserted {len(new_records_df)} new records\")\n                else:\n                    print(\"All records already exist\")\n                    \n        except Exception as e:\n            print(f\"Error during insert: {e}\")\n            loader.export(df, schema_name, table_name, index=False, if_exists='append', allow_reserved_words=True)\n    \n    print(\"Export completed\")", "file_path": "/home/src/scheduler/data_exporters/save_to_raw_customers_py.py", "language": "python", "type": "data_exporter", "uuid": "save_to_raw_customers_py"}, "/home/src/scheduler/data_exporters/save_to_raw_invoices.py:data_exporter:python:home/src/scheduler/data exporters/save to raw invoices": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\nimport pandas as pd\n\n@data_exporter\ndef save_to_raw_invoices(df: DataFrame, **kwargs) -> None:\n    schema_name = 'raw'\n    table_name = 'qb_invoices'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    \n    if len(df) == 0:\n        print(\"No records to process\")\n        return\n    \n    df = df[df['id'].notnull()]\n    df = df.drop_duplicates(subset=['id'])\n    \n    fecha_inicio = kwargs.get('fecha_inicio', '2025-01-01')\n    fecha_fin = kwargs.get('fecha_fin', '2025-09-16')\n    \n    timestamp_columns = ['ingested_at_utc', 'extract_window_start_utc', 'extract_window_end_utc']\n    \n    for col in timestamp_columns:\n        if col in df.columns:\n            if col == 'ingested_at_utc':\n                df[col] = pd.Timestamp.now(tz='UTC')\n            elif col == 'extract_window_start_utc':\n                df[col] = pd.Timestamp(fecha_inicio, tz='UTC')\n            elif col == 'extract_window_end_utc':\n                df[col] = pd.Timestamp(fecha_fin, tz='UTC')\n    \n    if 'page_number' in df.columns:\n        df['page_number'] = df['page_number'].astype(int)\n    if 'page_size' in df.columns:\n        df['page_size'] = df['page_size'].astype(int)\n    \n    if 'payload' in df.columns:\n        df['payload'] = df['payload'].astype(str)\n    if 'request_payload' in df.columns:\n        df['request_payload'] = df['request_payload'].astype(str)\n    \n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        try:\n            if len(df) > 0:\n                ids_str = \"','\".join(df['id'].astype(str).tolist())\n                existing_query = f\"SELECT id FROM {schema_name}.{table_name} WHERE id IN ('{ids_str}')\"\n                \n                existing_records = loader.load(existing_query)\n                existing_ids = set(existing_records['id'].tolist()) if len(existing_records) > 0 else set()\n                \n                new_records_df = df[~df['id'].isin(existing_ids)]\n                \n                if len(new_records_df) > 0:\n                    loader.export(new_records_df, schema_name, table_name, index=False, if_exists='append', allow_reserved_words=True)\n                    print(f\"Inserted {len(new_records_df)} new invoices\")\n                else:\n                    print(\"All invoices already exist\")\n                    \n        except Exception as e:\n            print(f\"Error during insert: {e}\")\n            loader.export(df, schema_name, table_name, index=False, if_exists='append', allow_reserved_words=True)\n    \n    print(\"Export completed\")", "file_path": "/home/src/scheduler/data_exporters/save_to_raw_invoices.py", "language": "python", "type": "data_exporter", "uuid": "save_to_raw_invoices"}, "/home/src/scheduler/data_exporters/save_to_raw_items_py.py:data_exporter:python:home/src/scheduler/data exporters/save to raw items py": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\nimport pandas as pd\n\n@data_exporter\ndef save_to_raw_items(df: DataFrame, **kwargs) -> None:\n    schema_name = 'raw'\n    table_name = 'qb_items'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    \n    if len(df) == 0:\n        print(\"No records to process\")\n        return\n    \n    df = df[df['id'].notnull()]\n    df = df.drop_duplicates(subset=['id'])\n    \n    fecha_inicio = kwargs.get('fecha_inicio', '2025-01-01')\n    fecha_fin = kwargs.get('fecha_fin', '2025-09-16')\n    \n    timestamp_columns = ['ingested_at_utc', 'extract_window_start_utc', 'extract_window_end_utc']\n    \n    for col in timestamp_columns:\n        if col in df.columns:\n            if col == 'ingested_at_utc':\n                df[col] = pd.Timestamp.now(tz='UTC')\n            elif col == 'extract_window_start_utc':\n                df[col] = pd.Timestamp(fecha_inicio, tz='UTC')\n            elif col == 'extract_window_end_utc':\n                df[col] = pd.Timestamp(fecha_fin, tz='UTC')\n    \n    if 'page_number' in df.columns:\n        df['page_number'] = df['page_number'].astype(int)\n    if 'page_size' in df.columns:\n        df['page_size'] = df['page_size'].astype(int)\n    \n    if 'payload' in df.columns:\n        df['payload'] = df['payload'].astype(str)\n    if 'request_payload' in df.columns:\n        df['request_payload'] = df['request_payload'].astype(str)\n    \n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        try:\n            if len(df) > 0:\n                ids_str = \"','\".join(df['id'].astype(str).tolist())\n                existing_query = f\"SELECT id FROM {schema_name}.{table_name} WHERE id IN ('{ids_str}')\"\n                \n                existing_records = loader.load(existing_query)\n                existing_ids = set(existing_records['id'].tolist()) if len(existing_records) > 0 else set()\n                \n                new_records_df = df[~df['id'].isin(existing_ids)]\n                \n                if len(new_records_df) > 0:\n                    loader.export(new_records_df, schema_name, table_name, index=False, if_exists='append', allow_reserved_words=True)\n                    print(f\"Inserted {len(new_records_df)} new items\")\n                else:\n                    print(\"All items already exist\")\n                    \n        except Exception as e:\n            print(f\"Error during insert: {e}\")\n            loader.export(df, schema_name, table_name, index=False, if_exists='append', allow_reserved_words=True)\n    \n    print(\"Export completed\")", "file_path": "/home/src/scheduler/data_exporters/save_to_raw_items_py.py", "language": "python", "type": "data_exporter", "uuid": "save_to_raw_items_py"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}